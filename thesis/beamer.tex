\documentclass[table]{beamer}

\usepackage{amssymb,amsmath,amsthm}
\usepackage{graphicx}
\usepackage{url}
\usepackage{hyperref}
\usepackage{epsfig}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{multirow}
\usepackage{array}
\usepackage{xcolor}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows}

\setbeamerfont{section title}{parent=title}
\setbeamercolor{section title}{parent=titlelike}
\defbeamertemplate*{section p}{default}[1][]
{
   \centering
     \begin{beamercolorbox}[sep=8pt,center,#1]{section title}
       \usebeamerfont{section title}\insertsection\par
     \end{beamercolorbox}
}
\newcommand*{\sectionp}{\usebeamertemplate*{section p}}

\title{SV-based regression techniques for survival analysis: a case study on veterinary data}
\author[Elvis Nava]{{\large Elvis Nava}\\[1ex]{\footnotesize \emph{Supervised by:}\\ Prof.\ Dario Malchiodi\\[-1ex] Prof.ssa Anna Maria Zanaboni}}
\date{Luglio 2018}
\institute[Unimi]{{\footnotesize Università degli Studi di Milano}\\[1ex] Facoltà di Scienze e Tecnologie\\ Corso di Laurea in Informatica}
\logo{\includegraphics[width=15mm]{unimi.jpg}}
\newcommand{\nologo}{\setbeamertemplate{logo}{}}

\usetheme{Frankfurt}
\makeatletter
\setbeamertemplate{headline}
{%
    \begin{beamercolorbox}[wd=\paperwidth,colsep=1.5pt]{upper separation line head}
    \end{beamercolorbox}
    \begin{beamercolorbox}[wd=\paperwidth,ht=2.5ex,dp=1.125ex,%
      leftskip=.3cm,rightskip=.3cm plus1fil]{title in head/foot}
      \usebeamerfont{title in head/foot}\insertshorttitle
    \end{beamercolorbox}
    \begin{beamercolorbox}[wd=\paperwidth,ht=2.5ex,dp=1.125ex,%
      leftskip=.3cm,rightskip=.3cm plus1fil]{section in head/foot}
      \usebeamerfont{section in head/foot}
      \insertsectionhead
    \end{beamercolorbox}
}
\makeatother

\begin{document}

\begin{frame}
\maketitle
\end{frame}

\begin{frame}
\frametitle{Table of Contents}
\tableofcontents
\end{frame}

\section{Support Vector Machines for Regression}
\frame{\sectionp}

\begin{frame}
\frametitle{Linear Support Vector Regression}
\begin{itemize}
\item From training data $ \mathbf{X} = \lbrace (x_{1},y_{1}),\ldots,(x_{\ell},y_{\ell})\rbrace \subset \mathcal{X} \times \mathbb{R} $, find the best $ f $ taking the form:
\begin{align*}
f(x) = \langle w,x \rangle + b \ \text{with} \ w \in \mathcal{X}, b \in \mathbb{R} \text{.}
\end{align*}
\item SVR training corresponds to the convex optimization problem:
\begin{align*}
\begin{split}
\text{minimize} &\ \dfrac{1}{2}\| w \|^2 + C\sum_{i=1}^{\ell}(\xi_{i} + \xi_{i}^{*}) \\
\text{subject to} &\ \begin{cases}
y_{i} - \langle w,x_{i} \rangle - b &\leq \varepsilon + \xi_{i} \text{,}\\
\langle w,x_{i} \rangle + b - y_{i} &\leq \varepsilon + \xi_{i}^{*} \text{,}\\
\xi_{i}, \xi_{i}^{*} &\geq 0 \text{.}
\end{cases}
\end{split}
\end{align*}
\end{itemize}
\end{frame}

{\nologo
\begin{frame}
\frametitle{Linear Support Vector Regression}
\begin{itemize}
\item SVR uses an $ \varepsilon $-insensitive loss function:
\begin{align*}
c(x,y,f(x)) = \vert y - f(x) \vert_{\varepsilon} = \begin{cases}
0 &\ \text{if } \vert y - f(x) \vert \leq \varepsilon \text{,}\\
\vert y - f(x) \vert - \varepsilon &\ \text{otherwise} \text{.}
\end{cases}
\end{align*}
\end{itemize}
\begin{figure}[h]
  	\centering
  	\includegraphics[width=0.49\textwidth]{figures/epstube.pdf}
  	\includegraphics[width=0.49\textwidth]{figures/epsloss.pdf}
\end{figure}
\end{frame}}

\begin{frame}
\frametitle{Linear Support Vector Regression}
\begin{itemize}
\item The optimization is performed on the Lagrangian dual:
\begin{scriptsize}
\begin{align*}
\begin{aligned}
\text{maximize} &\quad
-\dfrac{1}{2}\sum_{i,j=1}^{\ell}(\alpha_{i} - \alpha_{i}^{*})(\alpha_{j} - \alpha_{j}^{*})\langle x_{i},x_{j} \rangle -\varepsilon\sum_{i=1}^{\ell}(\alpha_{i} + \alpha_{i}^{*}) + \sum_{i=1}^{\ell}y_{i}(\alpha_{i}-\alpha_{i}^{*}) \\
\text{subject to} &\quad \left\{\begin{aligned}
&\sum_{i=1}^{\ell}(\alpha_{i} - \alpha_{i}^{*}) = 0 \text{,}\\
&\alpha_{i},\alpha_{i}^{*} \in [0,C] \text{.}
\end{aligned}\right.
\end{aligned}
\end{align*}
\end{scriptsize}
\item $ f $ can be written as a \textit{support vector expansion}
\begin{align*}
f(x) = \sum_{i=1}^{\ell}(\alpha_{i}-\alpha_{i}^{*})\langle x_{i},x\rangle + b \text{.}
\end{align*}
\item Each $ x_{i} $ for which $ (\alpha_{i}-\alpha_{i}^{*}) \neq 0 $ is called a \textit{support vector}.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Nonlinearity and kernels}
\begin{itemize}
\item The training points $ x_{i} $ can be preprocessed using a nonlinear map $ \Phi : \mathcal{X} \rightarrow \mathcal{F} $ into a higher-dimensional feature space $ \mathcal{F} $.
\item If a \textit{kernel} function $ k $ is known such that $ k(x_{i},x_{j}) = \langle\Phi (x_{i}),\Phi (x_{j})\rangle $, then it can be used instead of computing $ \Phi $ directly.
\item $k$ can be substituted into the Lagrangian dual and into $f$
\begin{align*}
f(x) = \sum_{i=1}^{\ell}(\alpha_{i}-\alpha_{i}^{*})k(x_{i},x) + b \text{.}
\end{align*}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Nonlinearity and kernels}
Examples of admissible kernels:
\begin{itemize}
\item The polynomial kernel \begin{align*}
k(x_{i},x_{j}) = (\gamma\langle x_{i},x_{j} \rangle + c)^{d}
\end{align*}
with $ d \in \mathbb{N} $, $ \gamma > 0 $ and $ c \geq 0 $.
\item The gaussian kernel \begin{align*}
k(x_{i},x_{j}) = e^{-\dfrac{\| x_{i}-x_{j} \|^{2}}{2\gamma^{2}}}
\end{align*}
with $ \gamma > 0 $.
\end{itemize}
\end{frame}

\section{Dataset Preprocessing and Exploration}
\frame{\sectionp}

{\nologo
\begin{frame}
\frametitle{The veterinary dataset ($ 161 \times 30$)}
Preliminary operations: a consistency check on the dataset, a correlation heatmap of the features, a series of joints plots with \textit{Survival time}.
\begin{figure}[h]
  \vspace{-0.2cm}
  \centering
  	\includegraphics[width=0.49\textwidth]{figures/heatmap.pdf}
  	\includegraphics[width=0.49\textwidth]{figures/jointplot3.pdf}
\end{figure}
\end{frame}}

\begin{frame}
\frametitle{Handling missing and censored data}
The following policies were experimented with in order to deal with NA values:
\begin{itemize}
\item Dropping NA values.
\item Using the mean value.
\item Sampling from a normal distribution.
\end{itemize}
\vspace{0.4cm}
The following elementary techniques have been considered to deal with the problem of the right-censoring of \textit{Survival time} values:
\begin{itemize}
\item Dropping censored values.
\item Using the maximum uncensored value.
\end{itemize}
\end{frame}

\section{Model Training and Selection}
\frame{\sectionp}

{\nologo
\begin{frame}
\frametitle{Feature trasformation, filtering and engineering}
\begin{itemize}
\item The data is scaled with feature scalers and optionally filtered for outliers (LOF).
\item New features can be added (a \textit{Therapy to visit} time $\Delta$) and highly correlated features can be identified and removed.
\end{itemize}
\begin{figure}[h]
  \centering
  	\includegraphics[width=0.49\textwidth]{figures/featsubset1.pdf}
  	\includegraphics[width=0.49\textwidth]{figures/featsubset2.pdf}
\end{figure}
\end{frame}}

\begin{frame}
\frametitle{The holdout learning pipeline}
\vspace{-0.2cm}
\begin{center}
\tikzstyle{block} = [rectangle, draw, fill=blue!20, text width=15em, text centered]
\tikzstyle{line} = [draw, -latex']
\tikzstyle{cloud} = [draw, rectangle, fill=blue!20, text width=3.5em, node distance=4.5cm, text centered]
\begin{tikzpicture}[node distance=1cm, auto]
	\node [block] (load) {Reading and preprocessing};
	\node [block, below of=load] (split1) {Training/Test split};
	\node [block, below of=split1] (split2) {Training/Validation split};
	\node [block, below of=split2] (scaling) {Scaling and (optional) LOF};
	\node [block, below of=scaling] (gridsearch) {Grid search on param grid};
	\node [cloud, left of=gridsearch] (hruns) {holdout runs left};
	\node [block, below of=gridsearch] (join) {Training/Validation rejoin};
	\node [block, below of=join] (report) {Test score returned};
	\node [cloud, right of=scaling] (oruns) {overall runs left};
	\node [block, below of=report] (avg) {Average test scores reported};
	\path [line] (load) -- (split1);
	\path [line] (split1) -- (split2);
	\path [line] (split2) -- (scaling);
	\path [line] (scaling) -- (gridsearch);
	\path [line] (gridsearch) -- (join);
	\path [line] (join) -- (report);
	\path [line] (join) -| (hruns);
	\path [line] (hruns) |- (split2);
	\path [line] (report) -| (oruns);
	\path [line] (oruns) |- (split1);
	\path [line] (report) -- (avg);
\end{tikzpicture}
\end{center}
\end{frame}

\section{Alternative SVR Models for Censored Datasets}
\frame{\sectionp}

{\nologo
\begin{frame}
\frametitle{SVCR and SVRC}
Modifications to the support vector machine model specifically proposed for working on censored datasets:
\begin{figure}[h]
  	\centering
  	\vspace{-0.5cm}
  	\begin{tabular}{ccc}
  		& \textbf{SVCR} & \textbf{SVRC} \\
  		\fbox{\footnotesize $\delta_i = 0$} &
  		\raisebox{-.5\height}{\includegraphics[width=0.4\textwidth]{figures/altloss2.pdf}} & 
  		\raisebox{-.5\height}{\includegraphics[width=0.4\textwidth]{figures/altloss4.pdf}} \\
  		\fbox{\footnotesize $\delta_i = 1$} &
		\raisebox{-.5\height}{\includegraphics[width=0.4\textwidth]{figures/altloss1.pdf}} &
		\raisebox{-.5\height}{\includegraphics[width=0.4\textwidth]{figures/altloss3.pdf}} \\
  	\end{tabular}
\end{figure}
\end{frame}}

\begin{frame}
\frametitle{Ranking-based alternative models}
A different approach to support vector learning for survival analysis is that of reformulating the survival prediction problem as a ranking problem.\\
This lets us introduce more models:
\begin{itemize}
\item \textbf{RankSVMC}, which consists in penalizing each comparable pair of data points for which the order of predicted indexes differs from the actual observed order.
\item \textit{Van Belle et al.} \textbf{Model 1}, a modified simplification of RankSVMC.
\item \textit{Van Belle et al.} \textbf{Model 2}, a combination of Model 1 and SVCR.
\end{itemize}
\end{frame}

\section{A Custom SVR Implementation}
\frame{\sectionp}

\begin{frame}
\frametitle{Technologies used}
\begin{itemize}
\item All the code was written in python, using libraries \texttt{numpy}, \texttt{pandas} and \texttt{scikit-learn}.
\item The dataset checks, explorations and the experiments were conducted with \texttt{jupyter} notebooks, rendering figures with \texttt{matplotlib} and \texttt{seaborn}.
\item The alternative SVR models have been implemented in the \texttt{customsvr} python module.
\item The Gurobi optimization framework was used for constructing and solving models. \texttt{customsvr} uses the Gurobi python API, also called \texttt{gurobipy}.
\end{itemize}
\end{frame}

{\nologo
\begin{frame}
\frametitle{The \texttt{customsvr} module}
Comparison between scikit-learn standard SVR and \texttt{customsvr}.
\begin{figure}[h]
  	\centering
  	\vspace{-0.5cm}
  	\begin{tabular}{ccc}
  		\hspace{-1cm}
  		\includegraphics[width=0.35\textwidth]{figures/custsvrcomp1.pdf} & 
		\includegraphics[width=0.35\textwidth]{figures/custsvrcomp2.pdf} &
		\includegraphics[width=0.35\textwidth]{figures/custsvrcomp3.pdf} \\
		\hspace{-1cm}
  		\includegraphics[width=0.35\textwidth]{figures/custsvrcomp4.pdf} &
		\includegraphics[width=0.35\textwidth]{figures/custsvrcomp5.pdf} &
		\includegraphics[width=0.35\textwidth]{figures/custsvrcomp6.pdf} \\
  	\end{tabular}
\end{figure}
\end{frame}}

\section{Results and Discussion}
\frame{\sectionp}

{\nologo
\begin{frame}
\frametitle{First SVR experiments}
{\scriptsize Experiment results on standard SVR model with repeated holdout, maximizing $R^2$ score.}
\begin{scriptsize}
\begin{center}
\vspace{-0.45cm}
\newcolumntype{g}{>{\columncolor{cyan!20}}c}
 \begin{tabular}{ |m{5cm}|g|c|c|c| }
\hline
\multicolumn{1}{|c|}{\multirow{2}{*}{Pipeline settings}} & \multicolumn{2}{|c|}{$R^2$} & \multicolumn{2}{|c|}{c-index} \\ \cline{2-5}
\rowcolor{white} & mean & sd & mean & sd \\ \hline
Drop NA, max censoring, StandardScaler & -1.0915 & 0.7466 & 0.6440 & 0.1266 \\ \hline
Mean NA, max censoring, StandardScaler & -1.6657 & 2.0754 & 0.6421 & 0.1193 \\ \hline
Normal generated NA, drop censoring, StandardScaler & -3.2109 & 3.5736 & 0.6133 & 0.1739 \\ \hline
Normal generated NA, max censoring, StandardScaler & -0.7714 & 0.9389 & 0.7005 & 0.0934 \\ \hline
Normal generated NA, max censoring, QuantileTransformer scaler & -0.7953 & 0.6020 & 0.5413 & 0.0680 \\ \hline
Normal generated NA, max censoring, StandardScaler, LOF outlier detector & -10.7221 & 12.3847 & 0.4537 & 0.1413 \\ \hline
Removing \textit{IP Gravity}, \textit{FE \%}, \textit{EDVI}, \textit{ESVI}, \textit{Allo sist}. Normal NA, max censoring, StandardScaler & -3.2217 & 6.0365 & 0.5750 & 0.0842 \\ \hline
Removing \textit{IP Gravity}, \textit{FS \%}, \textit{ESVI}, \textit{Allo diast} e \textit{Allo sist}. Normal NA, max censoring, StandardScaler & -1.4231 & 1.7689 & 0.5780 & 0.1549 \\ \hline
\end{tabular}
\end{center} 
\end{scriptsize}
\end{frame}}

\begin{frame}
\frametitle{Alternative models experiments}
Experiment results on alternative SVR models with repeated holdout, maximizing c-index score.
\begin{footnotesize}
\begin{center}
\newcolumntype{g}{>{\columncolor{yellow!40}}c}
 \begin{tabular}{ |m{5cm}|g|c|c|c| }
\hline
\multirow{2}{=}{SVR model with normal generated NA and StandardScaler} & \multicolumn{2}{|c|}{c-index} & \multicolumn{2}{|c|}{$R^2$} \\ \cline{2-5}
\rowcolor{white} & mean & sd & mean & sd \\ \hline
Standard SVR with max censoring & 0.5747 & 0.1593 & -1.8245 & 1.2600 \\ \hline
\rowcolor{yellow} SVCR & 0.6956 & 0.0699 & -1.8485 & 2.1166 \\ \hline
SVRC & 0.5368 & 0.1162 & -1.3478 & 1.5259 \\ \hline
RankSVMC (simplified) & 0.5855 & 0.1478 & - & - \\ \hline
Van Belle Model 1 & 0.5329 & 0.1100 & - & - \\ \hline
Van Belle Model 2 & 0.5233 & 0.2321 & -0.5028 & 0.6052 \\ \hline
\end{tabular}
\end{center} 
\end{footnotesize}
\end{frame}

\begin{frame}
\frametitle{Conclusions}
\vspace{-1cm}
\begin{itemize}
\item The ``standard'' Support Vector Regression approach proved insufficient as it was not designed to work with censored data. For this reason, a series of alternative SVR models were explored and implemented.
\item The dataset in question is not of sufficient size and quality for precise predictions of survival times to be made.
\item The use of SVR models for ranking observations and establishing risk groups was a success. The best performing model was \textbf{SVCR}, which introduces a modified loss function for censored targets.
\item The introduction of ranking-based methods such as \textbf{RankSVMC} and \textbf{Model 1}/\textbf{2} did not increase performance.
\end{itemize}
\end{frame}

\end{document}