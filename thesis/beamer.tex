\documentclass[table]{beamer}

\usepackage{amssymb,amsmath,amsthm}
\usepackage{graphicx}
\usepackage{url}
\usepackage{hyperref}
\usepackage{epsfig}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{multirow}
\usepackage{array}
\usepackage{xcolor}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows}

\setbeamerfont{section title}{parent=title}
\setbeamercolor{section title}{parent=titlelike}
\defbeamertemplate*{section p}{default}[1][]
{
   \centering
     \begin{beamercolorbox}[sep=8pt,center,#1]{section title}
       \usebeamerfont{section title}\insertsection\par
     \end{beamercolorbox}
}
\newcommand*{\sectionp}{\usebeamertemplate*{section p}}

\title{SV-based regression techniques for survival analysis: a case study on veterinary data}
\author[Elvis Nava]{{\large Elvis Nava}\\[1ex]{\footnotesize \emph{Relatori:}\\ Prof.\ Dario Malchiodi\\[-1ex] Prof.ssa Anna Maria Zanaboni}}
\date{10 Luglio 2018}
\institute[Unimi]{{\footnotesize Università degli Studi di Milano}\\[1ex] Facoltà di Scienze e Tecnologie\\ Corso di Laurea in Informatica}
\logo{\includegraphics[width=15mm]{unimi.jpg}}
\newcommand{\nologo}{\setbeamertemplate{logo}{}}

\usetheme{Frankfurt}
\makeatletter
\setbeamertemplate{headline}
{%
    \begin{beamercolorbox}[wd=\paperwidth,colsep=1.5pt]{upper separation line head}
    \end{beamercolorbox}
    \begin{beamercolorbox}[wd=\paperwidth,ht=2.5ex,dp=1.125ex,%
      leftskip=.3cm,rightskip=.3cm plus1fil]{title in head/foot}
      \usebeamerfont{title in head/foot}\insertshorttitle
    \end{beamercolorbox}
    \begin{beamercolorbox}[wd=\paperwidth,ht=2.5ex,dp=1.125ex,%
      leftskip=.3cm,rightskip=.3cm plus1fil]{section in head/foot}
      \usebeamerfont{section in head/foot}
      \insertsectionhead
    \end{beamercolorbox}
}
\makeatother

\begin{document}

\begin{frame}
\maketitle
\end{frame}

\section{Support Vector Machine per la Regressione}
\frame{\sectionp}

\begin{frame}
\frametitle{Support Vector Regression Lineare}
\begin{itemize}
\item A partire dai dati $ \mathbf{X} = \lbrace (x_{1},y_{1}),\ldots,(x_{\ell},y_{\ell})\rbrace \subset \mathcal{X} \times \mathbb{R} $, trova la migliore funzione $ f $ che abbia la forma
\begin{align*}
f(x) = \langle w,x \rangle + b \ \text{with} \ w \in \mathcal{X}, b \in \mathbb{R}
\end{align*}
\item Il training di SVR corrisponde al problema di ottimizzazione convessa
\begin{align*}
\begin{split}
\text{minimizza} &\ \dfrac{1}{2}\| w \|^2 + C\sum_{i=1}^{\ell}(\xi_{i} + \xi_{i}^{*}) \\
\text{soggetto a} &\ \begin{cases}
y_{i} - \langle w,x_{i} \rangle - b &\leq \varepsilon + \xi_{i}\\
\langle w,x_{i} \rangle + b - y_{i} &\leq \varepsilon + \xi_{i}^{*}\\
\xi_{i}, \xi_{i}^{*} &\geq 0
\end{cases}
\end{split}
\end{align*}
\end{itemize}
\end{frame}

{\nologo
\begin{frame}
\frametitle{Support Vector Regression Lineare}
\begin{itemize}
\item La SVR utilizza una ``$ \varepsilon $-insensitive loss function''
\begin{align*}
c(x,y,f(x)) = \vert y - f(x) \vert_{\varepsilon} = \begin{cases}
0 &\ \text{se } \vert y - f(x) \vert \leq \varepsilon\\
\vert y - f(x) \vert - \varepsilon &\ \text{altrimenti}
\end{cases}
\end{align*}
\end{itemize}
\begin{figure}[h]
  	\centering
  	\includegraphics[width=0.49\textwidth]{figures/epstube.pdf}
  	\includegraphics[width=0.49\textwidth]{figures/epsloss.pdf}
\end{figure}
\end{frame}}

\begin{frame}
\frametitle{Support Vector Regression Lineare}
\begin{itemize}
\item L'ottimizzazione è eseguita sul duale Lagrangiano
\begin{scriptsize}
\begin{align*}
\begin{aligned}
\text{massimizza} &\quad
-\dfrac{1}{2}\sum_{i,j=1}^{\ell}(\alpha_{i} - \alpha_{i}^{*})(\alpha_{j} - \alpha_{j}^{*})\langle x_{i},x_{j} \rangle -\varepsilon\sum_{i=1}^{\ell}(\alpha_{i} + \alpha_{i}^{*}) + \sum_{i=1}^{\ell}y_{i}(\alpha_{i}-\alpha_{i}^{*}) \\
\text{soggetto a} &\quad \left\{\begin{aligned}
&\sum_{i=1}^{\ell}(\alpha_{i} - \alpha_{i}^{*}) = 0\\
&\alpha_{i},\alpha_{i}^{*} \in [0,C]
\end{aligned}\right.
\end{aligned}
\end{align*}
\end{scriptsize}
\item $ f $ può essere scritta sottoforma di \textit{support vector expansion}
\begin{align*}
f(x) = \sum_{i=1}^{\ell}(\alpha_{i}-\alpha_{i}^{*})\langle x_{i},x\rangle + b
\end{align*}
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Nonlinearità e kernel}
\begin{itemize}
\item I punti di training $ x_{i} $ possono essere preelaborati con una mappa nonlineare $ \Phi : \mathcal{X} \rightarrow \mathcal{F} $ verso uno spazio delle feature sovradimensionato
\item Se una funzione $ k $ (detta \textit{kernel}) è conosciuta tale che $ k(x_{i},x_{j}) = \langle\Phi (x_{i}),\Phi (x_{j})\rangle $, allora può essere usata al posto di computare $ \Phi $ direttamente
\item $k$ può essere sostituita nel duale Lagrangiano e in $f$
\begin{align*}
f(x) = \sum_{i=1}^{\ell}(\alpha_{i}-\alpha_{i}^{*})k(x_{i},x) + b
\end{align*}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Nonlinearità e kernel}
Esempi di kernel ammissibili:
\begin{itemize}
\item Il kernel polinomiale \begin{align*}
k(x_{i},x_{j}) = (\gamma\langle x_{i},x_{j} \rangle + c)^{d}
\end{align*}
con $ d \in \mathbb{N} $, $ \gamma > 0 $ e $ c \geq 0 $.
\item Il kernel gaussiano \begin{align*}
k(x_{i},x_{j}) = e^{-\dfrac{\| x_{i}-x_{j} \|^{2}}{2\gamma^{2}}}
\end{align*}
con $ \gamma > 0 $.
\end{itemize}
\end{frame}

\section{Preprocessing ed Esplorazione del Dataset}
\frame{\sectionp}

{\nologo
\begin{frame}
\frametitle{Il dataset veterinario ($ 161 \times 30$)}
Operazioni preliminari: un controllo di coerenza, una heatmap di correlazione delle feature, una serie di diagrammi di dispersione con \textit{Survival time}.
\begin{figure}[h]
  \vspace{-0.2cm}
  \centering
  	\includegraphics[width=0.49\textwidth]{figures/heatmap.pdf}
  	\includegraphics[width=0.49\textwidth]{figures/jointplot3.pdf}
\end{figure}
\end{frame}}

\begin{frame}
\frametitle{Gestire dati mancanti o censurati}
Gestione dei valori NA
\begin{itemize}
\item Eliminazione dei valori NA
\item Utilizzo del valore medio
\item Generazione casuale da una distribuzione normale
\end{itemize}
\vspace{0.4cm}
Gestione dei valori censurati verso destra
\begin{itemize}
\item Eliminazione dei valori censurati
\item Utilizzo del massimo valore non censurato
\end{itemize}
\end{frame}

\section{Training e Selezione dei Modelli}
\frame{\sectionp}

{\nologo
\begin{frame}
\frametitle{Trasformazione, filtraggio ed engineering di feature}
\begin{itemize}
\item I dati sono scalati con opportuni feature scaler e opzionalmente filtrati per eliminare outlier (LOF)
\item Possono essere aggiunte nuove feature (un $\Delta$ temporale \textit{Therapy to visit}) e possono essere identificati e rimossi subset di feature altamente correlate
\end{itemize}
\begin{figure}[h]
  \centering
  	\includegraphics[width=0.49\textwidth]{figures/featsubset1.pdf}
  	\includegraphics[width=0.49\textwidth]{figures/featsubset2.pdf}
\end{figure}
\end{frame}}

\begin{frame}
\frametitle{La pipeline di apprendimento con holdout ripetuto}
\vspace{-0.7cm}
\begin{center}
\tikzstyle{block} = [rectangle, draw, fill=blue!20, text width=15em, text centered]
\tikzstyle{line} = [draw, -latex']
\tikzstyle{cloud} = [draw, ellipse, fill=blue!20, text width=3.9em, node distance=4.5cm, text centered]
\begin{tikzpicture}[node distance=1cm, auto]
	\node [block] (load) {Lettura e preprocessing};
	\node [block, below of=load] (split1) {Split di Training/Test set};
	\node [block, below of=split1] (split2) {Split di Training/Validation set};
	\node [block, below of=split2] (scaling) {Scaling e (opzionale) LOF};
	\node [block, below of=scaling] (gridsearch) {Grid search sui parametri};
	\node [cloud, left of=gridsearch] (hruns) {ripetizione holdout};
	\node [block, below of=gridsearch] (join) {Join di Training/Validation set};
	\node [block, below of=join] (report) {Ottenuto punteggio di test};
	\node [cloud, right of=scaling] (oruns) {ripetizione pipeline};
	\node [block, below of=report] (avg) {Ottenuta media dei test};
	\path [line] (load) -- (split1);
	\path [line] (split1) -- (split2);
	\path [line] (split2) -- (scaling);
	\path [line] (scaling) -- (gridsearch);
	\path [line] (gridsearch) -- (join);
	\path [line] (join) -- (report);
	\path [line] (join) -| (hruns);
	\path [line] (hruns) |- (split2);
	\path [line] (report) -| (oruns);
	\path [line] (oruns) |- (split1);
	\path [line] (report) -- (avg);
\end{tikzpicture}
\end{center}
\end{frame}

\section{Modelli SVR Alternativi per Dataset Censurati}
\frame{\sectionp}

{\nologo
\begin{frame}
\frametitle{SVCR ed SVRC}
Modifiche alla ``loss function'' del modello SVR proposte specificamente per gestire dataset censurati
\begin{figure}[h]
  	\centering
  	\vspace{-0.5cm}
  	\begin{tabular}{ccc}
  		& \textbf{SVCR} & \textbf{SVRC} \\
  		\fbox{\footnotesize $\delta_i = 0$} &
  		\raisebox{-.5\height}{\includegraphics[width=0.4\textwidth]{figures/altloss2.pdf}} & 
  		\raisebox{-.5\height}{\includegraphics[width=0.4\textwidth]{figures/altloss4.pdf}} \\
  		\fbox{\footnotesize $\delta_i = 1$} &
		\raisebox{-.5\height}{\includegraphics[width=0.4\textwidth]{figures/altloss1.pdf}} &
		\raisebox{-.5\height}{\includegraphics[width=0.4\textwidth]{figures/altloss3.pdf}} \\
  	\end{tabular}
\end{figure}
\end{frame}}

\begin{frame}
\frametitle{Modelli alternativi basati sul ranking}
Riformulazione della predizione dei tempi di sopravvivenza come problema di ranking
\begin{itemize}
\item \textbf{RankSVMC}, che penalizza ogni paio di punti comparabile per cui l'ordine di predizione è scorretto
\item \textit{Van Belle et al.} \textbf{Model 1}, una semplificazione di RankSVMC
\item \textit{Van Belle et al.} \textbf{Model 2}, una combinazione del precedente Model 1 e di SVCR
\end{itemize}
\end{frame}

\section{Un'Implementazione Custom di SVR}
\frame{\sectionp}

\begin{frame}
\frametitle{Tecnologie utilizzate}
\begin{itemize}
\item Il codice è stato scritto in python, usando le librerie \texttt{numpy}, \texttt{pandas} e \texttt{scikit-learn}
\item I controlli sul dataset, le esplorazioni e gli esperimenti sono stati condotti con i \texttt{jupyter} notebook, producendo le figure con \texttt{matplotlib} e \texttt{seaborn}
\item I modelli SVR alternativi sono stati implementati nel modulo python \texttt{customsvr}
\item Per costruire e risolvere i modelli è stato usato il framework di ottimizzazione Gurobi, tramite la python API \texttt{gurobipy}
\end{itemize}
\end{frame}

{\nologo
\begin{frame}
\frametitle{Il modulo \texttt{customsvr}}
Confronto tra la SVR standard di scikit-learn e \texttt{customsvr}
\begin{figure}[h]
  	\centering
  	\vspace{-0.5cm}
  	\begin{tabular}{ccc}
  		\hspace{-1cm}
  		\includegraphics[width=0.35\textwidth]{figures/custsvrcomp1.pdf} & 
		\includegraphics[width=0.35\textwidth]{figures/custsvrcomp2.pdf} &
		\includegraphics[width=0.35\textwidth]{figures/custsvrcomp3.pdf} \\
		\hspace{-1cm}
  		\includegraphics[width=0.35\textwidth]{figures/custsvrcomp4.pdf} &
		\includegraphics[width=0.35\textwidth]{figures/custsvrcomp5.pdf} &
		\includegraphics[width=0.35\textwidth]{figures/custsvrcomp6.pdf} \\
  	\end{tabular}
\end{figure}
\end{frame}}

\section{Risultati e Discussione}
\frame{\sectionp}

{\nologo
\begin{frame}
\frametitle{Primi esperimenti su SVR}
{\scriptsize Risultati degli esperimenti su SVR standard con holdout ripetuto, massimizzando $R^2$}
\begin{scriptsize}
\begin{center}
\vspace{-0.45cm}
\newcolumntype{g}{>{\columncolor{cyan!20}}c}
 \begin{tabular}{ |m{5cm}|g|c|c|c| }
\hline
\multicolumn{1}{|c|}{\multirow{2}{*}{Opzioni di pipeline}} & \multicolumn{2}{|c|}{$R^2$} & \multicolumn{2}{|c|}{c-index} \\ \cline{2-5}
& media & sd & media & sd \\ \hline
Drop NA, max censoring, StandardScaler & -1.0915 & 0.7466 & 0.6440 & 0.1266 \\ \hline
Mean NA, max censoring, StandardScaler & -1.6657 & 2.0754 & 0.6421 & 0.1193 \\ \hline
Normal generated NA, drop censoring, StandardScaler & -3.2109 & 3.5736 & 0.6133 & 0.1739 \\ \hline
Normal generated NA, max censoring, StandardScaler & -0.7714 & 0.9389 & 0.7005 & 0.0934 \\ \hline
Normal generated NA, max censoring, QuantileTransformer scaler & -0.7953 & 0.6020 & 0.5413 & 0.0680 \\ \hline
Normal generated NA, max censoring, StandardScaler, LOF outlier detector & -10.7221 & 12.3847 & 0.4537 & 0.1413 \\ \hline
Removing \textit{IP Gravity}, \textit{FE \%}, \textit{EDVI}, \textit{ESVI}, \textit{Allo sist}. Normal NA, max censoring, StandardScaler & -3.2217 & 6.0365 & 0.5750 & 0.0842 \\ \hline
Removing \textit{IP Gravity}, \textit{FS \%}, \textit{ESVI}, \textit{Allo diast} e \textit{Allo sist}. Normal NA, max censoring, StandardScaler & -1.4231 & 1.7689 & 0.5780 & 0.1549 \\ \hline
\end{tabular}
\end{center} 
\end{scriptsize}
\end{frame}}

\begin{frame}
\frametitle{Esperimenti su modelli alternativi}
Risultati degli esperimenti su modelli SVR alternativi con holdout ripetuto, massimizzando c-index
\begin{footnotesize}
\begin{center}
\newcolumntype{g}{>{\columncolor{yellow!40}}c}
 \begin{tabular}{ |m{5cm}|g|c|c|c| }
\hline
\multirow{2}{=}{SVR model with normal generated NA and StandardScaler} & \multicolumn{2}{|c|}{c-index} & \multicolumn{2}{|c|}{$R^2$} \\ \cline{2-5}
& media & sd & media & sd \\ \hline
Standard SVR with max censoring & 0.5747 & 0.1593 & -1.8245 & 1.2600 \\ \hline
\rowcolor{yellow} SVCR & 0.6956 & 0.0699 & -1.8485 & 2.1166 \\ \hline
SVRC & 0.5368 & 0.1162 & -1.3478 & 1.5259 \\ \hline
RankSVMC (simplified) & 0.5855 & 0.1478 & - & - \\ \hline
Van Belle Model 1 & 0.5329 & 0.1100 & - & - \\ \hline
Van Belle Model 2 & 0.5233 & 0.2321 & -0.5028 & 0.6052 \\ \hline
\end{tabular}
\end{center} 
\end{footnotesize}
\end{frame}

\begin{frame}
\frametitle{Conclusioni}
\vspace{-1cm}
\begin{itemize}
\item L'approccio Support Vector Regression ``standard'' si è dimostrato insufficiente, non essendo stato pensato per funzionare con dati censurati. Per questo motivo, sono stati considerati e implementati diversi modelli SVR alternativi.
\item Il dataset in questione non è di dimensione e qualità sufficiente per ottenere predizioni precise sui tempi di sopravvivenza.
\item L'uso di modelli SVR per il ranking delle osservazioni e la definizione di gruppi di rischio si è rivelato un successo. Il modello più performante è \textbf{SVCR}, che introduce una loss function modificata.
\item L'introduzione di modelli che ottimizzano il ranking come \textbf{RankSVMC} e \textbf{Model 1}/\textbf{2} non ha aumentato la performance.
\end{itemize}
\end{frame}

\end{document}