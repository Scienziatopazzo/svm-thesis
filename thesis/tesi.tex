\documentclass[12pt]{report}

\usepackage[a4paper]{geometry}
\usepackage{amssymb,amsmath,amsthm}
\usepackage{graphicx}
\usepackage{url}
\usepackage{hyperref}
\usepackage{epsfig}
\usepackage[english]{babel}
\usepackage{tesi}
\usepackage[nottoc]{tocbibind}

\usepackage[utf8]{inputenc}

\newtheorem{myteor}{Theorem}[section]
\newenvironment{teor}{\begin{myteor}\sl}{\end{myteor}}

\begin{document}
\title{SV-based regression techniques for survival analysis: a case study on veterinary data}
\author{Elvis NAVA}
\dept{Corso di Laurea in Informatica} 
\anno{2017-2018}
\matricola{870234}
\relatore{Dario MALCHIODI}
\correlatore{Anna Maria ZANABONI}
%
%        \submitdate{month year in which submitted to GPO}
%		- date LaTeX'd if omitted
%	\copyrightyear{year degree conferred (next year if submitted in Dec.)}
%		- year LaTeX'd (or next year, in December) if omitted
%	\copyrighttrue or \copyrightfalse
%		- produce or don't produce a copyright page (false by default)
%	\figurespagetrue or \figurespagefalse
%		- produce or don't produce a List of Figures page
%		  (false by default)
%	\tablespagetrue or \tablespagefalse
%		- produce or don't produce a List of Tables page
%		  (false by default)
% 
%
\beforepreface
\topskip0pt
\vspace*{\fill}
{\hfill \Large {\sl dedicated to \dots}}
\vspace*{\fill}
% 
%			PREFACE
%
\prefacesection{Preface}
This is a preface.
%
%			ACKNOWLEDGMENTS
%
\prefacesection{Acknowledgments}

\afterpreface


\chapter*{Introduction}
\label{intro}
\addcontentsline{toc}{chapter}{Introduction}
The subject of this thesis is the use of regression techniques, specifically support vector machine regression, in the analysis and prediction of survival times. The techniques are applied to a dataset of dogs' veterinary records, with the goal of modeling survival from the details of medical visits. As with most other similar datasets, the right-censoring of survival times requires a modification to the standard support vector machine algorithm in order to be dealt with. For this reason, this work includes the writing of a custom SVM implementation.

\subsection*{Thesis structure}
The thesis begins in chapter \ref{chsvm} with an overview of function estimation and risk minimization, followed by a description of support vector machine regression for both the linear and the nonlinear case.

In chapter \ref{chprepr} the veterinary dataset subject of this thesis is first presented, then explored after a number of preprocessing steps, performed to guarantee data consistency and deal with problems such as NA (not available) values.

Chapter \ref{chmodsel} is about model selection, achieved by tuning the hyperparameters of the support vector regression algorithm. Feature engineering is also considered as a way of obtaining a better performing regression model.

\ldots TODO

\subsection*{Technologies used}

\chapter{Support Vector Machines for Regression}
\label{chsvm}
Survival analysis can be modeled as a regression problem, survival time being a continuous variable dependent on a certain number of covariates. This chapter begins with an introduction to function estimation in general, followed by a description of support vector machine regression, a machine learning technique used to solve a particular formulation of linear and non-linear regression by framing it as an optimization problem. A tutorial on the basics of support vector machines is available in \cite{svmtutorial}, while a description of their use for regression can be found in \cite{svregtutorial}.

\section{Function estimation and risk minimization}
Given a set of training data $ \mathbf{X} := \lbrace (x_{1},y_{1}),\ldots,(x_{\ell},y_{\ell})\rbrace \subset \mathcal{X} \times \mathbb{R} $ and assuming it has been drawn (independent and identically distributed) from a probability distribution $ P(x,y) $, the goal of function estimation is to find the function $ f $ minimizing the expectation of test error, also known as risk functional:
\begin{equation} \label{riskfun}
R[f] = \int c(x,y,f(x))dP(x,y)
\end{equation}
$ c(x,y,f(x)) $ being a loss function that determines the penalization for estimation errors. However, the distribution $ P(x,y) $ is not known, and therefore $ R[f] $ cannot be computed. Using $ \mathbf{X} $, it's only possible to compute the empirical risk functional:
\begin{equation} \label{empriskfun}
R_{emp}[f] = \dfrac{1}{\ell} \sum_{i=1}^{\ell}c(x_{i},y_{i},f(x_{i}))
\end{equation}
which makes no reference to the original probability distribution. Nonetheless, simply minimizing $ R_{emp}[f] $ for some function class $ H $ is not a good strategy, as generally it is not true that $ \operatorname*{argmin}_{f} R_{emp}[f] = \operatorname*{argmin}_{f} R[f] $.

Different bounds for $ R[f] $ based on $ R_{emp}[f] $ have been demonstrated to hold. In the context of binary classification, an example is the Vapnik-Chervonenkis inequality \cite{vapnik95}:
\begin{equation} \label{vapnikbound}
R[f] \leq R_{emp}[f] + \sqrt{\dfrac{h(\log (2\ell /h) + 1) - \log (\eta /4)}{\ell}}
\end{equation}
valid with a probability of $ 1 - \eta $, with $ \eta $ such that $ 0 \leq \eta \leq 1 $. $ h $ is called the \textit{VC dimension} of the function class $ H $ from which $ f $ is chosen, and it is a measure of the \textit{"capacity"} of the function class. When the "capacity" of $ H $ is very high, the VC dimension increases, leading in this example to a higher upper bound on the risk (in the reasonable case of $ h \leq \ell $), and in practice to a propensity towards overfitting and bad performance.

One solution to this problem, useful even outside of classification, is to add a capacity control term to the empirical risk, so that $ H $ is artificially kept small. In the case of support vector machines, the function $ f $ is determined based on a vector of weights $ w $. By using $ \| w \|^2 $ as the capacity control term added to the expression being minimized, the number of different functions obtainable is reduced. This leads to the regularized risk functional:
\begin{equation} \label{regriskfun}
R_{reg}[f] = R_{emp}[f] + \dfrac{\lambda}{2}\| w \|^2
\end{equation}
where $ \lambda > 0 $ is a regularization constant.

\section{Linear support vector regression}
Given training data $ \mathbf{X} \subset \mathcal{X} \times \mathbb{R} $, the goal of hard-margin support vector regression is to find the \textit{flattest} function $ f $ such that, for every $ (x_{i},y_{i}) \in \mathbf{X} $, $ f(x_{i}) $ has at most $ \varepsilon $ deviation from the actual target $ y_{i} $. In the linear case, $ f $ takes the form:
\begin{equation} \label{linfun}
f(x) = \langle w,x \rangle + b \ \text{with} \ w \in \mathcal{X}, b \in \mathbb{R}
\end{equation}
In order to control function class capacity, $ \| w \|^2 $ is minimized, and in this sense the "flattest" function is obtained. This can be written as a convex optimization problem:
\begin{equation} \label{hmargprimal}
\begin{split}
\text{minimize} &\ \dfrac{1}{2}\| w \|^2 \\
\text{subject to} &\ \begin{cases}
y_{i} - \langle w,x_{i} \rangle - b &\leq \varepsilon \\
\langle w,x_{i} \rangle + b - y_{i} &\leq \varepsilon
\end{cases}
\end{split}
\end{equation}
This "hard margin" formulation may sometimes be unfeasible, or lead to overfitting in the case of noisy training data. For this reason, soft-margin support vector regression allows for errors by introducing slack variables $ \xi_{i} $, $ \xi_{i}^{*} $, penalized in the objective function by the coefficient $ C > 0 $:
\begin{equation} \label{smargprimal}
\begin{split}
\text{minimize} &\ \dfrac{1}{2}\| w \|^2 + C\sum_{i=1}^{\ell}(\xi_{i} + \xi_{i}^{*}) \\
\text{subject to} &\ \begin{cases}
y_{i} - \langle w,x_{i} \rangle - b &\leq \varepsilon + \xi_{i} \\
\langle w,x_{i} \rangle + b - y_{i} &\leq \varepsilon + \xi_{i}^{*} \\
\xi_{i}, \xi_{i}^{*} &\geq 0
\end{cases}
\end{split}
\end{equation}
In terms of loss functions for risk minimization, this corresponds to using an $ \varepsilon $-insensitive loss function:
\begin{equation} \label{epsloss}
c(x,y,f(x)) = \vert y - f(x) \vert_{\varepsilon} = \begin{cases}
0 &\ \text{if } \vert y - f(x) \vert \leq \varepsilon \\
\vert y - f(x) \vert - \varepsilon &\ \text{otherwise}
\end{cases}
\end{equation}
so that predictions laying inside the $ \varepsilon $-insensitive tube are not penalized, while those laying outside are penalized linearly. This is different from the squared distance loss function used in standard least squares regression.

The optimization problem in (\ref{smargprimal}), referred to as the \textit{primal} problem, is more easily solved in its \textit{dual} formulation. This is constructed from the Lagrangian of the objective function:
\begin{equation} \label{lagrangian}
\begin{split}
L =& \ \dfrac{1}{2}\| w \|^2 + C\sum_{i=1}^{\ell}(\xi_{i} + \xi_{i}^{*}) - \sum_{i=1}^{\ell}(\eta_{i}\xi_{i} + \eta_{i}^{*}\xi_{i}^{*}) \\
&- \sum_{i=1}^{\ell}\alpha_{i}(\varepsilon + \xi_{i} - y_{i} + \langle w,x_{i} \rangle + b) \\
&- \sum_{i=1}^{\ell}\alpha_{i}^{*}(\varepsilon + \xi_{i}^{*} + y_{i} - \langle w,x_{i} \rangle - b)
\end{split}
\end{equation}
with $ \eta_{i} $, $ \eta_{i}^{*} $, $ \alpha_{i} $, $ \alpha_{i}^{*} $ being positive Lagrange multipliers.

Because of the saddle point coindition, the partial derivatives of $ L $ with respect to the primal variables must be equal to zero at optimality:
\begin{align}
\label{partb}
\partial_{b}L &= \sum_{i=1}^{\ell}(\alpha_{i}^{*} - \alpha_{i}) = 0 \\
\label{partw}
\partial_{w}L &= w - \sum_{i=1}^{\ell}(\alpha_{i} - \alpha_{i}^{*})x_{i} = 0 \\
\label{partxi}
\partial_{\xi_{i}^{(*)}}L &= C - \alpha_{i}^{(*)} - \eta_{i}^{(*)} = 0
\end{align}
Substituting (\ref{partb}), (\ref{partw}), and (\ref{partxi}) into (\ref{lagrangian}) leads to the dual optimization problem:
\begin{equation} \label{smargdual}
\begin{split}
\text{maximize} &\quad
-\dfrac{1}{2}\sum_{i,j=1}^{\ell}(\alpha_{i} - \alpha_{i}^{*})(\alpha_{j} - \alpha_{j}^{*})\langle x_{i},x_{j} \rangle -\varepsilon\sum_{i=1}^{\ell}(\alpha_{i} + \alpha_{i}^{*}) + \sum_{i=1}^{\ell}y_{i}(\alpha_{i}-\alpha_{i}^{*}) \\
\text{subject to} &\quad \left\{\begin{split}
&\sum_{i=1}^{\ell}(\alpha_{i} - \alpha_{i}^{*}) = 0 \\
&\alpha_{i},\alpha_{i}^{*} \in [0,C]
\end{split}\right.
\end{split}
\end{equation}
Equation (\ref{partw}) can be rewritten as $ w = \sum_{i=1}^{\ell}(\alpha_{i}-\alpha_{i}^{*})x_{i} $, this means $ w $ can be fully represented as a linear combination of the training points $ x_{i} $. Therefore, the function $ f $ obtained from the solution to this optimization problem can be written as a \textit{support vector expansion}:
\begin{equation} \label{fsvexp}
f(x) = \sum_{i=1}^{\ell}(\alpha_{i}-\alpha_{i}^{*})\langle x_{i},x\rangle + b
\end{equation}
Each point $ x_{i} $ for which $ (\alpha_{i}-\alpha_{i}^{*}) \neq 0 $ is called a \textit{support vector}, and only these points are needed for a sparse representation of $ f $.  Because of this, the complexity of $ f $ only depends on the number of support vectors, and not on the dimensionality of $ \mathcal{X} $.

\section{Nonlinearity and kernels}
In order to extend support vector machine regression to the nonlinear case, the training points $ x_{i} $ can be preprocessed using a map $ \Phi : \mathcal{X} \rightarrow \mathcal{F} $ into some higher-dimensional feature space $ \mathcal{F} $, and then the standard algorithm can be used. For example, preprocessing the inputs with the map $ \Phi : \mathbb{R}^{2} \rightarrow \mathbb{R}^{3} $ such that $ \Phi(x_{1},x_{2}) = (x_{1}^{2},\sqrt{2}x_{1}x_{2},x_{2}^{2}) $ yields a quadratic function $ f $ after training.

However, simple preprocessing can quickly become computationally unfeasible as the dimensionality of the transformed data fed to the algorithm increases. The key observation leading to a computationally cheaper way of obtaining nonlinearity lies in the fact that the support vector machine algorithm never directly makes use of the input points $ x_{i} $, but only of their dot products $ \langle x_{i},x_{j} \rangle $. This means that if a function $ k $ is known such that $ k(x_{i},x_{j}) = \langle\Phi (x_{i}),\Phi (x_{j})\rangle $, then it can be used instead of computing $ \Phi $ directly. $ k $ is called a \textit{kernel} function, and is substituted into the optimization problem:
\begin{equation} \label{kernelsmargdual}
\begin{split}
\text{maximize} &\quad
-\dfrac{1}{2}\sum_{i,j=1}^{\ell}(\alpha_{i} - \alpha_{i}^{*})(\alpha_{j} - \alpha_{j}^{*})k(x_{i},x_{j}) -\varepsilon\sum_{i=1}^{\ell}(\alpha_{i} + \alpha_{i}^{*}) + \sum_{i=1}^{\ell}y_{i}(\alpha_{i}-\alpha_{i}^{*}) \\
\text{subject to} &\quad \left\{\begin{split}
&\sum_{i=1}^{\ell}(\alpha_{i} - \alpha_{i}^{*}) = 0 \\
&\alpha_{i},\alpha_{i}^{*} \in [0,C]
\end{split}\right.
\end{split}
\end{equation}
$ f $ and $ w $ are likewise changed:
\begin{equation} \label{wnonlinear}
w = \sum_{i=1}^{\ell}(\alpha_{i}-\alpha_{i}^{*})\Phi(x_{i})
\end{equation}
\begin{equation} \label{kernelfsvexp}
f(x) = \sum_{i=1}^{\ell}(\alpha_{i}-\alpha_{i}^{*})k(x_{i},x) + b
\end{equation}
with the difference from the linear case being that $ w $ is no longer computed explicitly, while $ f $ only needs its support vectors $ x_{i} $ in their original low-dimensional form to be computed.

An example of an ammissible kernel is the polynomial kernel:
\begin{equation} \label{polykernel}
k(x_{i},x_{j}) = (\langle x_{i},x_{j} \rangle + \gamma)^{d}
\end{equation}
with $ d \in \mathbb{N} $ and $ \gamma \geq 0 $, which corresponds to feature spaces $ \mathcal{F} $ yielding different kinds of polynomial functions $ f $.

Another example is the gaussian kernel:
\begin{equation} \label{gausskernel}
k(x_{i},x_{j}) = e^{-\dfrac{\| x_{i}-x_{j} \|^{2}}{2\gamma^{2}}}
\end{equation}
which corresponds to the feature spaces $ \mathcal{F} $ of radial basis functions. In this case, $ \mathcal{F} $ is actually infinite-dimensional, so directly computing $ \Phi $ would have been impossible.

\chapter{Dataset Preprocessing and Exploration}
\label{chprepr}
The real world application of support vector regression explored in this thesis consists in a work of survival analysis on a veterinary dataset. Before any meaningful analysis can be carried out, it's important to apply a number of checks and alterations to the raw dataset, so that it becomes suitable for processing. This kind of work, called \textit{preprocessing}, is paramount to successful analysis of real world data. Following these procedures, an initial exploration of the dataset can be made.

\section{Dataset specifics}
The dataset, provided as a \texttt{.xlsx} file without formulas, is composed of a collection of dogs' veterinary records, separated in 3 sheets: \textit{2006-2016} data $ (161 \times 30) $, \textit{2001-2005} data $ (69 \times 30) $, and a \textit{legend}. Both data sheets have 30 columns, corresponding to the features enumerated in the legend, but only the one from the \textit{2006-2016} period actually uses all of them. Therefore, only the first sheet has been deemed fit for analysis, hereafter referred to as the \texttt{dogs\_2006\_2016} dataset, with a shape of $ (161\; data\; points \times 30\; features) $.

\section{Feature legend}
Here is the list of all features:
\begin{itemize}
\item \textit{Folder}: patient folder ID;
\item \textit{IP}: presence of Pulmonary Insufficiency (0 = no, 1 = yes);
\item \textit{IP Gravity}: gravity of Pulmonary Insufficiency (0 = absent, 1 = minor, 2 = moderate, 3 = severe);
\item \textit{Vrig Tric}: tricuspid regurgitation velocity;
\item \textit{Birth date}: birth date of patient;
\item \textit{First visit}: date of first medical visit;
\item \textit{Age}: age of patient;
\item \textit{Therapy started}: date of therapy beginning;
\item \textit{Dead}: death of patient during treatment (0 = no, 1 = yes);
\item \textit{Date of death}: date of death, right-censored for alive patients;
\item \textit{MC}: death due to cardiac arrest (0 = no, 1 = yes);
\item \textit{Survival time}: survival time from first visit, right-censored for alive patients;
\item \textit{Furosemide}: prescription of furosemide (0 = no, 1 = yes);
\item \textit{Ache-i}: prescription of acetylcholinesterase inhibitors (0 = no, 1 = yes);
\item \textit{Pimobendan}: prescription of pimobendan (0 = no, 1 = yes);
\item \textit{Spironolattone}: prescription of spironolactone (0 = no, 1 = yes);
\item \textit{Therapy Category}: category of prescriptions, obtained as a sum of the values of features \textit{Furosemide}, \textit{Ache-i}, \textit{Pimobendan} and \textit{Spironolattone};
\item \textit{Antiaritmico}: prescription of antiarrhythmic (0 = no, 1 = yes);
\item \textit{isachc}: ISACHC classification of patient;
\item \textit{Class}: ACVIM classification of patient;
\item \textit{Weight (Kg)}: weight of patient in kilograms;
\item \textit{Asx/Ao}: left atrium / aortic root ratio;
\item \textit{E}: E-wave;
\item \textit{E/A}:	E-wave / A-wave ratio;
\item \textit{FE \%}: ejection fraction;
\item \textit{FS \%}: shortening fraction;
\item \textit{EDVI}: End-diastolic volume of left ventricle indexed for body surface area;
\item \textit{ESVI}: End-systolic volume of left ventricle indexed for body surface area;
\item \textit{Allo diast}: End-diastolic volume of left ventricle not indexed for body surface area;
\item \textit{Allo sist}: End-systolic volume of left ventricle not indexed for body surface area.
\end{itemize}

\section{Consistency check}
The \texttt{dogs\_2006\_2016} dataset was provided without any in-built formulas, so it was deemed sensible to try a number of consistency checks on dependent features, such as dates and times.
\subsection*{Date consistency}
The first check involved testing the validity of the inequality $ Birth\; date \leq First\; visit \leq Therapy\; started \leq Date\; of\; death $. This check produced 42 errors, all from data points for which $ Therapy\; started < First\; visit $, while in all the remaining cases the two values were equal. Then it became clear that the correct interpretation of the data corresponds in fact to the inequality $ Birth\; date \leq Therapy\; started \leq First\; visit \leq Date\; of\; death $. \textit{Therapy started} is equal to \textit{First visit} when the therapy started on the first visit, while it is set to an earlier date if the dog was already undergoing therapy under different supervision.
\subsection*{Survival time consistency}
A check of the \textit{Survival time} as calculated from \textit{First visit} to \textit{Date of death} revealed only one erroneous data point, with an error delta of exactly one year. Since the value was supposedly obtained automatically, the origin of this error is unclear. A decision was made to fix this error in all future analyses by using by default the value obtained from the dates.
\subsection*{Cardiac arrest and death}
A check was performed on whether or not the dataset is consistent with regard to the implication $ CardiacDeath \rightarrow Dead $, although for the purpose of this work any information related to the dog's death such as a cardiac arrest must not be included in the actual prediction models. The check was anyways passed with no reported inconsistencies.
\subsection*{Therapy category}
The \textit{Therapy category} feature was supposedly calculated as the sum of various binary features, each representing a pharmaceutical prescription. A check was made to ensure its correctness and no errors were reported.
\subsection*{Age}
A check on the \textit{Age} value was meant to determine whether it was recorded on the first visit, or on another occasion. Instead, the findings suggest that the value has no consistent correlation with any date feature. Calculating the value from the first visit date yields an error on 50 data points, while changing it to the death date only fixes 5 of these errors. In the end, it was deemed sensible to simply use the newly calculated values obtained with the first visit date, instead of the reported ones.

\section{Exploratory analysis}
After the first checks necessary to assess data consistency, the dataset was further inspected using some exploratory analysis tools. The insights obtained with this techniques proved useful for subsequent tasks such as feature selection and normalization.
\subsection*{Feature correlation}
A correlation heatmap of the available features was used to highlight possible dependencies among them. In particular, Fig. XX shows that the following groups of features exhibit a relevant correlation:
\begin{itemize}
\item \textit{IP Gravity} and \textit{Vrig tric}: a subsequent test confirmed that the first feature is indeed a discretization of the second one;
\item \textit{Dead} and \textit{MC} (Cardiac arrest death);
\item \textit{FE \%} and \textit{FS \%};
\item \textit{EDVI}, \textit{ESVI}, \textit{Allo diast} and \textit{Allo sist} (especially \textit{EDVI} with \textit{Allo diast} and \textit{ESVI} with \textit{Allo sist}).
\end{itemize}
\subsection*{Scatter plots with \textit{Survival time} and other features}
\textit{Survival time} was plotted against all the other features. As shown in Fig. YY, no obvious patterns were found, except for a slight correlation between \textit{Age} and \textit{Asx/ao}.

\section{Handling missing data}
The considered dataset, as is the case with most real world data, contains NA (Not Available) values that need to be handled in some way before being used as input for model training. The following policies were experimented with in order to deal with this problem:
\begin{itemize}
\item \textbf{Dropping NA values}. This procedure consists in simply deleting all data points that include NA values in any feature field. It does not introduce potentially spurious information, but effectively reduces the dataset's size.
\item \textbf{Using the mean value}. This procedure consists in filling any NA value with the mean value of all data points for the corresponding feature. It maintains the dataset's size but it might alter the distributions of features, especially when the number of NA values is high.
\item \textbf{Sampling from a normal distribution}. This technique assumes a normal distribution for the involved features, and replaces NAs with values drawn from an approximation of such distributions. It maintains the dataset's size and feature distributions, but introduces an element of randomness in the analysis. Before applying this technique it's important to verify the normality of the affected features using a tool such as a QQ-plot.
\item \textbf{Using values predicted from a model (e.g., Linear Regression)}. TODO
\end{itemize}

\section{Handling censored data}
A crucial problem in survival analysis is that of data censoring: in this case the \textit{Survival time} feature is right-censored for observations for which the subject is registered as alive. This means that the reported survival time is just a lower bound, while the real time is unknown, either because the subject died after the last visit or because it's still alive.

This problem, discussed more in depth later in this thesis, inspired a modification of the standard Support Vector Machine model. However, some elementary techniques can be applied even at this stage of analysis. Precisely, the following methodologies have been considered:
\begin{itemize}
\item \textbf{Drop censored values}. simply delete all data points for dogs that have not yet died. This technique removes the need to handle censoring in the analysis, but deletes important information at the same time.
\item \textbf{Using the maximum non-censored value}. find the maximum \textit{Survival time} value for dogs that did die and substitute it to all censored values. This technique represents a rudimentary way of letting alive subjects positively influence survival time prediction, as their value is artificially set at the maximum in the resulting dataset.
\end{itemize}

\chapter{Model Training and Selection}
\label{chmodsel}
Training a learning algorithm on actual data brings about many issues.

Firstly, even after preprocessing the dataset, the features have to be scaled so that they are not too differently distributed and they don't cause numerical problems. It may even be useful to remove outliers from the training data. Furthermore, the number and nature of feature themselves may need to be altered in order to obtain a better performing model, in what is called \textit{feature engineering}.

But most importantly, models have to be scored and selected, as algorithms such as support vector machines depend on a number of hyperparameters that cannot be determined a priori. Common techniques for model selection are holdout and K-fold cross-validation.

\section{The training pipeline}

\section{Cross-validation and repeated holdout}

\section{Feature engineering}


\chapter{Alternative SVM Models for Censored Datasets}
\label{chaltsvm}

\chapter{A Custom SVM Implementation}
\label{chcustsvm}

\chapter{Results and Discussion}
\label{chres}

\chapter*{Conclusions}
\label{concl}
\addcontentsline{toc}{chapter}{Conclusions}

%
%			BIBLIOGRAPHY
%
\begin{thebibliography}{00}

\bibitem{svmtutorial}
Burges, Christopher JC. "A tutorial on support vector machines for pattern recognition." Data mining and knowledge discovery 2.2 (1998): 121-167.

\bibitem{svregtutorial}
Smola, Alex J., and Bernhard Schölkopf. "A tutorial on support vector regression." Statistics and computing 14.3 (2004): 199-222.

\bibitem{vapnik95}
Vapnik V., "The Nature of Statistical Learning Theory." Springer, New York (1995).

%
\end{thebibliography}
% 
\end{document}


 
